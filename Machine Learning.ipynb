{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a machine learning model\n",
    "<br />\n",
    "<br />\n",
    "  In this Notebook i will train a machine learning algorithm to perform a sentiment analysis. I use a dataset i found on Kaggle.com with Tweets that have already been annotated with the apporiate sentiment. The first step is a little exploratory data analysis to see how the data is strucutred. Then i will apply various preprocessing steps to clean the data. Finally i will train some ML algorithms and try to determine under which condidtions they perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pycld2 as cld2\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                                              tweet\n",
       "0       0  1467810672  is upset that he can't update his Facebook by ...\n",
       "1       0  1467810917  @Kenichan I dived many times for the ball. Man...\n",
       "2       0  1467811184    my whole body feels itchy and like its on fire \n",
       "3       0  1467811193  @nationwideclass no, it's not behaving at all....\n",
       "4       0  1467811372                      @Kwesidei not the whole crew "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.columns = ['target', \"id\", \"date\", \"flag\", \"username\", \"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.drop(columns=['date', 'flag','username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "4         800000\n",
       "0         799999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.value_counts(['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<br />\n",
    "<br />\n",
    "First i will remove contractions and replace them to provide better results and more consistant usage of the language and and reduce the noise a little bit. Also popular abbreviations like 'u' for you and '4' for 'for' are replaced too.  \n",
    "Then i clean the tweets: i remove @mentions completly because they add no valuable data. Hashtags (the symbol itself) are removed but not the actual hashtags itself. Hyperlinks and special characters are also removed as well as more than 2 characters.  \n",
    "In the next step i remove all the stop words.\n",
    "Finally the verbs are lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace common contractions and abbreviations\n",
    "def repl_contract(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    cont = {\n",
    "        \"aren't\" : 'are not',\n",
    "        \"can't\" : 'cannot',\n",
    "        \"couldn't\" : 'could not',\n",
    "        \"don't\" : \"do not\",\n",
    "        \"didn't\" : 'did not',\n",
    "        \"doesn't\" : 'does not',\n",
    "        \"hadn't\" : 'had not',\n",
    "        \"haven't\" : 'have not',\n",
    "        \"hasn't\" : \"has not\",\n",
    "        \"how's\" : \"how is\",\n",
    "        \"he's\" : 'he is',\n",
    "        \"she's\" : 'she is',\n",
    "        \"he'll\" : \"he will\",        \n",
    "        \"she'll\" : 'she will',\n",
    "        \"he'd\" : \"he would\",\n",
    "        \"she'd\" : \"she would\",\n",
    "        \"here's\" : \"here is\", \n",
    "        \"i'm\" : 'i am',\n",
    "        \"i've\" : \"i have\",\n",
    "        \"i'll\" : \"i will\",\n",
    "        \"i'd\" : \"i would\",\n",
    "        \"isn't\": \"is not\", \n",
    "        \"it's\" : \"it is\",\n",
    "        \"it'll\" : \"it will\",\n",
    "        \"mustn't\" : \"must not\",\n",
    "        \"shouldn't\" : \"should not\",\n",
    "        \"that's\" : \"that is\", \n",
    "        \"there's\" : \"there is\",\n",
    "        \"they're\" : \"they are\",\n",
    "        \"they've\" : \"they have\",\n",
    "        \"they'll\" : \"they will\",\n",
    "        \"they'd\" : \"they would\",\n",
    "        \"wasn't\" : \"was not\",\n",
    "        \"we're\" : \"we are\",\n",
    "        \"we've\" : \"we have\",\n",
    "        \"we'll\" : \"we will\", \n",
    "        \"we'd\" : \"we would\",\n",
    "        \"weren't\" : \"were not\",\n",
    "        \"what's\" : \"what is\",\n",
    "        \"when's\" : \"when is\",\n",
    "        \"why's\" : \"why is\",\n",
    "        \"where's\" : \"where is\",\n",
    "        \"who's\" : \"who is\",\n",
    "        \"who'll\" : \"who will\",\n",
    "        \"won't\" : \"will not\",\n",
    "        \"wouldn't\" : \"would not\",\n",
    "        \"you're\" : \"you are\",\n",
    "        \"you've\" : \"you have\",\n",
    "        \"you'll\" : \"you will\",\n",
    "        \"you'd\" : \"you would\",\n",
    "        \"mayn't\" : \"may not\",\n",
    "        \"4\" : \"for\",\n",
    "        \"2\" : \"to\",\n",
    "        \"1\" : \"one\",\n",
    "        \"u\" : \"you\",\n",
    "        \"r\" : \"are\",\n",
    "        \"amp\" : \"\",\n",
    "        \"re\" : \"\",\n",
    "        \"gimme\" : \"give me\",\n",
    "        \"gonna\" : \"going to\",\n",
    "        \"cause\" : \"because\",\n",
    "        \"imma\" : \"i am going to\",\n",
    "        \"wanna\" : \"want to\",\n",
    "        \"gotta\" : \"got to\",\n",
    "        \"woulda\" : \"would have\",\n",
    "        \"coulda\" : \"could have\",\n",
    "        \"shoulda\" : \"should have\",\n",
    "        \"let's\" : \"let us\",\n",
    "        \"y'all\" : \"you all\",\n",
    "            }\n",
    "    \n",
    "    cleaned_text = []\n",
    "    words = text.split()\n",
    "    \n",
    "    for word in words:\n",
    "        if word in cont:\n",
    "            cleaned_text.append(cont[word])\n",
    "        else:\n",
    "            cleaned_text.append(word)\n",
    "    text = ' '.join(cleaned_text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the tweets\n",
    "def clean(text):\n",
    "    text = text.lower() # lower case\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\",\"\", text) # remove @mentions\n",
    "    text = re.sub(r\"#\",\"\", text) # remove #\n",
    "    text = re.sub(r\"\\ART[\\s]+\",\"\", text) # remove RT\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\",\"\", text) # remove hyperlink\n",
    "    text = re.sub(\"(.)\\\\1{2,}\",\"\\\\1\", text) # remove more than two characters.\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s]+\", \" \",str(text)) #remove special characters\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove stopwords\n",
    "def remv_stopw(text):\n",
    "\n",
    "    cleaned_text = []\n",
    "    words = word_tokenize(text) \n",
    "\n",
    "    for word in words:\n",
    "        if not word in stop_words:\n",
    "                cleaned_text.append(word)\n",
    "            \n",
    "    return ' '.join(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize verbs \n",
    "def lemma(text, stem=False, lemmatize=False):\n",
    "\n",
    "    cleaned_text = []\n",
    "    words = word_tokenize(text) \n",
    "\n",
    "    for word in words:\n",
    "        cleaned_text.append(lemmatizer.lemmatize(word,pos=\"v\"))\n",
    "            \n",
    "    return ' '.join(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect language\n",
    "def detect_lang(tweet):\n",
    "    try:\n",
    "        isReliable, textBytesFound, details = cld2.detect(tweet)\n",
    "        return details[0][0]\n",
    "    except:\n",
    "        return \"not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save various steps of my cleaned data to test if the algorithms benefit from removing stop words and lemmatizing.\n",
    "\n",
    " + `clean_tweet`= Replaced contractions and common abbreviations as well as removed hyperlinks, special characters and etc.\n",
    " + `clean_tweet_nostop`= Removed stop words also.  \n",
    " - `clean_tweet_nostop_lemma`= Lemmatized the verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['clean_tweet'] = training_data['tweet'].apply(repl_contract)\n",
    "training_data['clean_tweet'] = training_data['clean_tweet'].apply(clean)\n",
    "training_data['clean_tweet_nostop'] = training_data['clean_tweet'].apply(remv_stopw)\n",
    "training_data['clean_tweet_nostop_lemma'] = training_data['clean_tweet_nostop'].apply(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>clean_tweet_nostop</th>\n",
       "      <th>clean_tweet_nostop_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cannot update his facebook by...</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>i dived many times for the ball  managed to s...</td>\n",
       "      <td>dived many times ball managed save 50 rest go ...</td>\n",
       "      <td>dive many time ball manage save 50 rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no  it is not behaving at all  i am mad  why ...</td>\n",
       "      <td>behaving mad see</td>\n",
       "      <td>behave mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>whole crew</td>\n",
       "      <td>whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                                              tweet  \\\n",
       "0       0  1467810672  is upset that he can't update his Facebook by ...   \n",
       "1       0  1467810917  @Kenichan I dived many times for the ball. Man...   \n",
       "2       0  1467811184    my whole body feels itchy and like its on fire    \n",
       "3       0  1467811193  @nationwideclass no, it's not behaving at all....   \n",
       "4       0  1467811372                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  is upset that he cannot update his facebook by...   \n",
       "1   i dived many times for the ball  managed to s...   \n",
       "2     my whole body feels itchy and like its on fire   \n",
       "3   no  it is not behaving at all  i am mad  why ...   \n",
       "4                                 not the whole crew   \n",
       "\n",
       "                                  clean_tweet_nostop  \\\n",
       "0  upset update facebook texting might cry result...   \n",
       "1  dived many times ball managed save 50 rest go ...   \n",
       "2                   whole body feels itchy like fire   \n",
       "3                                   behaving mad see   \n",
       "4                                         whole crew   \n",
       "\n",
       "                            clean_tweet_nostop_lemma  \n",
       "0  upset update facebook texting might cry result...  \n",
       "1   dive many time ball manage save 50 rest go bound  \n",
       "2                    whole body feel itchy like fire  \n",
       "3                                     behave mad see  \n",
       "4                                         whole crew  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['language'] = training_data['clean_tweet'].apply(detect_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(\" \".join(training_data['language']).split()).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['language'].replace({'SCOTS': 'ENGLISH'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0         768278\n",
       "4         757892\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data[training_data[\"language\"] == \"ENGLISH\"]\n",
    "training_data.value_counts(['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now my data is skewed a little. Thats why i take a even number of positive and negative tweets to get the best results for my algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "4         750000\n",
       "0         750000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg = training_data.loc[training_data['target'] == 0]\n",
    "pos = training_data.loc[training_data['target'] == 4]\n",
    "\n",
    "pos = pos[0:750000]\n",
    "neg = neg[0:750000]\n",
    "\n",
    "training_data = pos.append(neg,ignore_index = True)\n",
    "training_data.value_counts(['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>clean_tweet_nostop</th>\n",
       "      <th>clean_tweet_nostop_lemma</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822272</td>\n",
       "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
       "      <td>i love  you guys are the best</td>\n",
       "      <td>love guys best</td>\n",
       "      <td>love guy best</td>\n",
       "      <td>ENGLISH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822273</td>\n",
       "      <td>im meeting up with one of my besties tonight! ...</td>\n",
       "      <td>im meeting up with one of my besties tonight  ...</td>\n",
       "      <td>im meeting one besties tonight cant wait girl ...</td>\n",
       "      <td>im meet one besties tonight cant wait girl talk</td>\n",
       "      <td>ENGLISH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822283</td>\n",
       "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
       "      <td>thanks for the twitter add  sunisa  i got to ...</td>\n",
       "      <td>thanks twitter add sunisa got meet hin show dc...</td>\n",
       "      <td>thank twitter add sunisa get meet hin show dc ...</td>\n",
       "      <td>ENGLISH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822287</td>\n",
       "      <td>Being sick can be really cheap when it hurts t...</td>\n",
       "      <td>being sick can be really cheap when it hurts t...</td>\n",
       "      <td>sick really cheap hurts much eat real food plu...</td>\n",
       "      <td>sick really cheap hurt much eat real food plus...</td>\n",
       "      <td>ENGLISH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822293</td>\n",
       "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
       "      <td>he has that effect on everyone</td>\n",
       "      <td>effect everyone</td>\n",
       "      <td>effect everyone</td>\n",
       "      <td>ENGLISH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                                              tweet  \\\n",
       "0       4  1467822272       I LOVE @Health4UandPets u guys r the best!!    \n",
       "1       4  1467822273  im meeting up with one of my besties tonight! ...   \n",
       "2       4  1467822283  @DaRealSunisaKim Thanks for the Twitter add, S...   \n",
       "3       4  1467822287  Being sick can be really cheap when it hurts t...   \n",
       "4       4  1467822293    @LovesBrooklyn2 he has that effect on everyone    \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0                     i love  you guys are the best    \n",
       "1  im meeting up with one of my besties tonight  ...   \n",
       "2   thanks for the twitter add  sunisa  i got to ...   \n",
       "3  being sick can be really cheap when it hurts t...   \n",
       "4                     he has that effect on everyone   \n",
       "\n",
       "                                  clean_tweet_nostop  \\\n",
       "0                                     love guys best   \n",
       "1  im meeting one besties tonight cant wait girl ...   \n",
       "2  thanks twitter add sunisa got meet hin show dc...   \n",
       "3  sick really cheap hurts much eat real food plu...   \n",
       "4                                    effect everyone   \n",
       "\n",
       "                            clean_tweet_nostop_lemma language  \n",
       "0                                      love guy best  ENGLISH  \n",
       "1    im meet one besties tonight cant wait girl talk  ENGLISH  \n",
       "2  thank twitter add sunisa get meet hin show dc ...  ENGLISH  \n",
       "3  sick really cheap hurt much eat real food plus...  ENGLISH  \n",
       "4                                    effect everyone  ENGLISH  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "<br />\n",
    "<br />\n",
    "In this part i train various machine learning algorithms and evalute their accuracy.  \n",
    "The algortihms i used are:\n",
    "\n",
    " + Linear Regressio\n",
    " - Passive Aggressive\n",
    " - Ridge Regression\n",
    " - Linear Support Vector Machines\n",
    " - ADA Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_LR(train_data, targets):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train_data, targets, test_size=0.1)\n",
    "\n",
    "    model = Pipeline([('vect', HashingVectorizer(ngram_range=(1, 2))),\n",
    "                      ('clf', LogisticRegression(max_iter=1000, solver=\"saga\")),\n",
    "              ])\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_PA(train_data, targets):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train_data, targets, test_size=0.2)\n",
    "\n",
    "    model = Pipeline([('vect', HashingVectorizer(ngram_range=(2, 2))),\n",
    "                      ('clf', PassiveAggressiveClassifier()),\n",
    "              ])\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_RR(train_data, targets):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train_data, targets, test_size=0.2)\n",
    "\n",
    "    model = Pipeline([('vect', HashingVectorizer(ngram_range=(2, 2))),\n",
    "                      ('clf', RidgeClassifier()),\n",
    "              ])\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_SVC(train_data, targets):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train_data, targets, test_size=0.2)\n",
    "\n",
    "    model = Pipeline([('vect', HashingVectorizer(ngram_range=(2, 2))),\n",
    "                      ('clf', LinearSVC(random_state=0, tol=1e-5)),\n",
    "              ])\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ADA(train_data, targets):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train_data, targets, test_size=0.2)\n",
    "\n",
    "    model = Pipeline([('vect', HashingVectorizer(ngram_range=(1, 2))),\n",
    "                      ('clf', AdaBoostClassifier(n_estimators=100, random_state=0)),\n",
    "              ])\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_metrics(model, test_data, test_targets):\n",
    "    y_pred = model.predict(test_data)\n",
    "\n",
    "    print(\"ACCURACY:\")\n",
    "    print(metrics.accuracy_score(test_targets, y_pred)*100)\n",
    "\n",
    "    print(\"\\nCONFUSION MATRIX\")\n",
    "    print(confusion_matrix(test_targets, y_pred))\n",
    "\n",
    "    print(\"\\nCLASSIFICATION REPORT\")\n",
    "    print(classification_report(test_targets, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different sets of training data for testing the algorithms.\n",
    "-clean_tweet = removed contractions and punctuation etc..\n",
    "-clean_tweet_nostop = removed stopwords\n",
    "-clean_tweet_nostop_lemma = leammtized and no stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = training_data['clean_tweet']\n",
    "train_data_2 = training_data['clean_tweet_nostop']\n",
    "train_data_3 = training_data['clean_tweet_nostop_lemma']\n",
    "targets = training_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:\n",
      "82.074\n",
      "\n",
      "CONFUSION MATRIX\n",
      "[[60938 14194]\n",
      " [12695 62173]]\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82     75132\n",
      "           4       0.81      0.83      0.82     74868\n",
      "\n",
      "    accuracy                           0.82    150000\n",
      "   macro avg       0.82      0.82      0.82    150000\n",
      "weighted avg       0.82      0.82      0.82    150000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_LR, x_test, y_test = train_model_LR(train_data_1, targets)\n",
    "check_model_metrics(model_LR, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR, x_test, y_test = train_model_LR(train_data_2, targets)\n",
    "check_model_metrics(model_LR, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR, x_test, y_test = train_model_LR(train_data_3, targets)\n",
    "check_model_metrics(model_LR, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Agressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PA, x_test, y_test = train_model_PA(train_data_1, targets)\n",
    "check_model_metrics(model_PA, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PA, x_test, y_test = train_model_PA(train_data_2, targets)\n",
    "check_model_metrics(model_PA, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PA, x_test, y_test = train_model_PA(train_data_3, targets)\n",
    "check_model_metrics(model_PA, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RR, x_test, y_test = train_model_RR(train_data_1, targets)\n",
    "check_model_metrics(model_RR, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RR, x_test, y_test = train_model_RR(train_data_2, targets)\n",
    "check_model_metrics(model_RR, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RR, x_test, y_test = train_model_RR(train_data_3, targets)\n",
    "check_model_metrics(model_RR, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVC, x_test, y_test = train_model_SVC(train_data_1, targets)\n",
    "check_model_metrics(model_SVC, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVC, x_test, y_test = train_model_SVC(train_data_2, targets)\n",
    "check_model_metrics(model_SVC, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVC, x_test, y_test = train_model_SVC(train_data_3, targets)\n",
    "check_model_metrics(model_SVC, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADA Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ADA, x_test, y_test = train_model_ADA(train_data_1, targets)\n",
    "check_model_metrics(model_ADA, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ADA, x_test, y_test = train_model_ADA(train_data_2, targets)\n",
    "check_model_metrics(model_ADA, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ADA, x_test, y_test = train_model_ADA(train_data_3, targets)\n",
    "check_model_metrics(model_ADA, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the algorithms perform best on the data set that didn't have the stop words removed as well as no lemmatization. So i will use the same apporoach on the election tweets.\n",
    "\n",
    "\n",
    "I save the three best performing algorithms in a pickle file for later use. The best one (=model_LR) is used for the sentiment analysis of the election tweets in my other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('model_LR.pickle','wb')\n",
    "pickle.dump(model_LR, file)\n",
    "file.close()\n",
    "\n",
    "file = open('model_SVC.pickle','wb')\n",
    "pickle.dump(model_SVC, file)\n",
    "file.close()\n",
    "\n",
    "file = open('model_RC.pickle','wb')\n",
    "pickle.dump(model_RC, file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
